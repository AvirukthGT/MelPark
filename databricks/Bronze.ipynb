{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042811e3-e2ff-4270-b3d2-6fb0e8d7ffe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG melpark_azure;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbefc92-f603-4e3d-8ca0-4f6a51476da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct connection configured for: abfss://bronze@melpark.dfs.core.windows.net\n"
     ]
    }
   ],
   "source": [
    "# 1. Get your Storage Key\n",
    "storage_account = \"melpark\" \n",
    "storage_key = dbutils.secrets.get(scope=\"kv-scope-v1\", key=\"storage-account-key\")\n",
    "\n",
    "# 2. Authorize Spark to read directly (Bypassing /mnt)\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", storage_key)\n",
    "\n",
    "# 3. Define the Direct Paths (ABFSS)\n",
    "# This format is: abfss://<container>@<account>.dfs.core.windows.net/<folder>\n",
    "base_uri = f\"abfss://bronze@{storage_account}.dfs.core.windows.net\"\n",
    "\n",
    "print(f\"Direct connection configured for: {base_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c343bc-558a-4606-98ce-a854fd2f9909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting parking_bays from abfss://bronze@melpark.dfs.core.windows.net/parking_bays...\nSuccess! Managed table 'melpark_azure.bronze.parking_bays' updated.\nIngesting parking_meters from abfss://bronze@melpark.dfs.core.windows.net/parking_meters...\nSuccess! Managed table 'melpark_azure.bronze.parking_meters' updated.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def ingest_static_table(table_name, folder_name):\n",
    "    # Construct the direct path\n",
    "    source_path = f\"{base_uri}/{folder_name}\"\n",
    "    print(f\"Ingesting {table_name} from {source_path}...\")\n",
    "    \n",
    "    # 1. READ from Direct Path\n",
    "    df = spark.read.format(\"parquet\").load(source_path)\n",
    "    \n",
    "    # 2. Add Metadata\n",
    "    df_final = df.withColumn(\"ingested_at\", current_timestamp())\n",
    "    \n",
    "    # 3. WRITE to Unity Catalog\n",
    "    target_table = f\"melpark_azure.bronze.{table_name}\"\n",
    "    \n",
    "    df_final.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(target_table)\n",
    "    \n",
    "    print(f\"Success! Managed table '{target_table}' updated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a597d280-b69b-46de-8766-844b1e27462b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting parking_restrictions from abfss://bronze@melpark.dfs.core.windows.net/parking_restrictions...\nSuccess! Managed table 'melpark_azure.bronze.parking_restrictions' updated.\nIngesting parking_zones_plates from abfss://bronze@melpark.dfs.core.windows.net/parking_zones_plates...\nSuccess! Managed table 'melpark_azure.bronze.parking_zones_plates' updated.\n"
     ]
    }
   ],
   "source": [
    "# Run ingestion\n",
    "ingest_static_table(\"parking_bays\", \"parking_bays\")\n",
    "ingest_static_table(\"parking_meters\", \"parking_meters\")\n",
    "ingest_static_table(\"parking_restrictions\",\"parking_restrictions\")\n",
    "ingest_static_table(\"parking_zones_plates\", \"parking_zones_plates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e991cf-f408-4c10-aaf5-82d571c7206e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Auto Loader stream to melpark_azure.bronze.parking_sensors...\nStream is active!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Configuration\n",
    "source_path = f\"{base_uri}/topics/parking.public.raw_parking_sensors\"\n",
    "checkpoint_path = f\"{base_uri}/_checkpoints/parking_sensors_uc\"\n",
    "# FIX: Define a path for the schema master copy\n",
    "schema_path = f\"{base_uri}/_schemas/parking_sensors\" \n",
    "\n",
    "table_name = \"melpark_azure.bronze.parking_sensors\"\n",
    "\n",
    "print(f\"Starting Auto Loader stream to {table_name}...\")\n",
    "\n",
    "# 1. Read Stream\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    # FIX: Add this required option\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path) \n",
    "    .load(source_path)\n",
    ")\n",
    "\n",
    "# 2. Add Metadata\n",
    "df_stream_final = df_stream.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "# 3. Write Stream\n",
    "query = (df_stream_final.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\") \n",
    "    .table(table_name)\n",
    ")\n",
    "\n",
    "print(\"Stream is active!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd7a0b7-f601-435f-86a0-145dfc3a7d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5548725085566787,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}